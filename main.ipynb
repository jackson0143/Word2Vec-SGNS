{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a70d4439",
   "metadata": {},
   "source": [
    "## Word2Vec from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748ffc8d",
   "metadata": {},
   "source": [
    "We will be building a Word2Vec method\n",
    "- What it is? It helps create the vector representation of a given word, called word embeddings.\n",
    "- Why is it useful? The vectors we create will aim to capture semantic meanings and their relationships with different words, so the famous example 'king' and 'queen' will end up in a similar vector space.\n",
    "- It might feel like a FFNN, it is the same structure but an embedding model instead of a classification one, it learns word embeddings (semantics in words)\n",
    "\n",
    "There's two ways to do it:\n",
    "1. Continuous Bag of Words (CBOW) which tries to predict a word in the sentence, given its surrounding neighbour words.\n",
    "eg: the quick brown _____ jumps over the lazy dog. We can try to use our surrounding words of 'brown' and 'jumps' to try to predict the missing word 'fox'.\n",
    "\n",
    "2. Skip Gram is the reverse, instead it will predict the neighbours of a given word. \n",
    "eg: the quick ______ fox ______ over the lazy dog.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd73f1c",
   "metadata": {},
   "source": [
    "## Step 1: setup the dataset\n",
    "\n",
    "I'll go with the reddit text corpus today from Convokit. Its supposed stats are:\n",
    "\n",
    "- Number of Utterances: 297132\n",
    "- Number of Speakers: 119889\n",
    "- Number of Conversations: 8286\n",
    "\n",
    "We'll import the dataset then see what we're working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b8b8a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerDecoderModel requires ML dependencies. Run 'pip install convokit[llm]' to install them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Eunha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerEncoderModel requires ML dependencies. Run 'pip install convokit[llm]' to install them.\n",
      "UnslothUtteranceSimulatorModel requires ML dependencies. Run 'pip install convokit[llm]' to install them.\n",
      "Dataset already exists at C:\\Users\\Eunha\\.convokit\\saved-corpora\\reddit-corpus-small\n"
     ]
    }
   ],
   "source": [
    "from convokit import Corpus, download\n",
    "corpus = Corpus(filename=download(\"reddit-corpus-small\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "641c9a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 utterances loaded\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "from typing import Dict, List, Sequence, Tuple\n",
    "MAX_UTTERANCES = 10000\n",
    "chosen_utts = [utt.text for utt in islice(corpus.iter_utterances(), MAX_UTTERANCES)]\n",
    "\n",
    "print(len(chosen_utts), \"utterances loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b0fbfc",
   "metadata": {},
   "source": [
    "Okay we have chosen one line, lets proceed with working with this line to get a feel of how this works. Lets tokenise it to ensure we remove any ambiguities in variations of words that might trip up the model. eg: lowercase the words so 'Talk' and 'talk' aren't differentiated during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de8a2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def tokenise(text: str) -> List[str]:\n",
    "    return re.findall(r\"[a-z0-9]+'[a-z0-9]+|[a-z0-9]+\", text.lower(), flags=re.I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43f32ca",
   "metadata": {},
   "source": [
    "Now with each utterance cleaned and broken down into simple tokens, we can map it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330b4de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gloss\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(tokenised_utts):\n",
    "    # Flatten tokenised utterances into a single list of words\n",
    "    words: List[str] = [token for utt in tokenised_utts for token in utt]\n",
    "    vocab = set(words)\n",
    "    word_to_idx:Dict[int, str] = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx_to_word:Dict[int, str] = {idx: word for word, idx in word_to_idx.items()}\n",
    "    return words, word_to_idx, idx_to_word\n",
    "\n",
    "\n",
    "def tokens_to_ids(tokenised_utts: List[List[str]], word_to_idx: Dict[str, int]) -> List[List[int]]:\n",
    "    return [[word_to_idx[w] for w in utt if w in word_to_idx] for utt in tokenised_utts]\n",
    "\n",
    "tokenised_utts = [tokenise(utt) for utt in chosen_utts]\n",
    "words, word_to_idx, idx_to_word = build_vocab(tokenised_utts)\n",
    "\n",
    "print(idx_to_word[105])\n",
    "\n",
    "utt_ids = tokens_to_ids(tokenised_utts, word_to_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aa0120",
   "metadata": {},
   "source": [
    "# Step 2: Define the Skip-gram pairs. \n",
    "\n",
    "Lets say for example we had a window size of 2:\n",
    "\n",
    "tokens = [\"quick\", \"brown\", \"fox\", \"jumps\", \"over\"]\n",
    "                0        1      2       3        4\n",
    "\n",
    "For this iteration, we choose 'fox' as the center word.\n",
    "window size tells us how far left and right we can look for the context words, so we can make 2 steps up until index 0 or index 4.\n",
    "\n",
    "Then all the words within index 0 and index 4 will be considered our **context** words (not including center word).\n",
    "Then we just make pairs for all of them, with the pairs being **(center, context)**\n",
    "\n",
    "Eg:(fox, quick), (fox, brown), (fox, jumps), (fox, over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cef84b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "def make_skip_gram_pairs(tokens: Sequence[int], window: int, dynamic_window: bool = False) -> List[Tuple[int, int]]:\n",
    "    pairs: List[Tuple[int, int]] = []\n",
    "    for i, center in enumerate(tokens):\n",
    "        w = random.randint(1, window) if dynamic_window else window\n",
    "        left_pointer = max(0, i - w)\n",
    "        right_pointer = min(len(tokens) - 1, i + w)\n",
    "        for j in range(left_pointer, right_pointer + 1):\n",
    "            if j == i:\n",
    "                continue\n",
    "            pairs.append((center, tokens[j]))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def make_pairs_for_all_utterances(utt_ids: List[List[int]], window: int, dynamic_window: bool = False) -> List[Tuple[int, int]]:\n",
    "    all_pairs: List[Tuple[int, int]] = []\n",
    "    for utt in utt_ids:\n",
    "        if not utt:\n",
    "            continue\n",
    "        all_pairs.extend(make_skip_gram_pairs(utt, window, dynamic_window))\n",
    "    return all_pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e685504",
   "metadata": {},
   "source": [
    "We laod the data into a dataloader from PyTorch, so we can run as batches later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e960e178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, data: List[Tuple[int, int]]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[int, int]:\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "WINDOW = 2\n",
    "DYNAMIC_WINDOW = True\n",
    "BATCH_SIZE = 256\n",
    "training_data = make_pairs_for_all_utterances(utt_ids, WINDOW, DYNAMIC_WINDOW)\n",
    "dataset = Word2VecDataset(training_data)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892e2a60",
   "metadata": {},
   "source": [
    "# Step 4: Construct the actual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8372458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        #embedding lookup tables/matrix, essentially map words to the embedding vectors\n",
    "        self.input_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.output_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, center_indices: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        center_vecs = self.input_embeddings(center_indices) \n",
    "        scores = center_vecs @ self.output_embeddings.weight.t()\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c657228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 17650, Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_to_idx)\n",
    "\n",
    "#this is a hyperparam, configure as needed\n",
    "embedding_dim = 50\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SkipGram(vocab_size=vocab_size, embedding_dim=embedding_dim).to(device)\n",
    "\n",
    "#standard loss fn and optimiser\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(f\"Vocab size: {vocab_size}, Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1da73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/5 ===\n",
      "[Epoch 1 | Step 1] loss=29.8818\n",
      "[Epoch 1 | Step 500] loss=24.1885\n",
      "[Epoch 1 | Step 1000] loss=22.0120\n",
      "[Epoch 1 | Step 1500] loss=20.4950\n",
      "[Epoch 1 | Step 2000] loss=19.3149\n",
      "[Epoch 1 | Step 2500] loss=18.3104\n",
      "[Epoch 1 | Step 3000] loss=17.4336\n",
      "[Epoch 1 | Step 3500] loss=16.6655\n",
      "Epoch 1 completed. Avg loss: 16.6264\n",
      "\n",
      "=== Epoch 2/5 ===\n",
      "[Epoch 2 | Step 1] loss=10.3970\n",
      "[Epoch 2 | Step 500] loss=11.0480\n",
      "[Epoch 2 | Step 1000] loss=10.7564\n",
      "[Epoch 2 | Step 1500] loss=10.4984\n",
      "[Epoch 2 | Step 2000] loss=10.2817\n",
      "[Epoch 2 | Step 2500] loss=10.0879\n",
      "[Epoch 2 | Step 3000] loss=9.9032\n",
      "[Epoch 2 | Step 3500] loss=9.7355\n",
      "Epoch 2 completed. Avg loss: 9.7267\n",
      "\n",
      "=== Epoch 3/5 ===\n",
      "[Epoch 3 | Step 1] loss=8.7862\n",
      "[Epoch 3 | Step 500] loss=8.3544\n",
      "[Epoch 3 | Step 1000] loss=8.2742\n",
      "[Epoch 3 | Step 1500] loss=8.1990\n",
      "[Epoch 3 | Step 2000] loss=8.1274\n",
      "[Epoch 3 | Step 2500] loss=8.0652\n",
      "[Epoch 3 | Step 3000] loss=8.0023\n",
      "[Epoch 3 | Step 3500] loss=7.9446\n",
      "Epoch 3 completed. Avg loss: 7.9423\n",
      "\n",
      "=== Epoch 4/5 ===\n",
      "[Epoch 4 | Step 1] loss=7.5636\n",
      "[Epoch 4 | Step 500] loss=7.3568\n",
      "[Epoch 4 | Step 1000] loss=7.3340\n",
      "[Epoch 4 | Step 1500] loss=7.3086\n",
      "[Epoch 4 | Step 2000] loss=7.2838\n",
      "[Epoch 4 | Step 2500] loss=7.2613\n",
      "[Epoch 4 | Step 3000] loss=7.2380\n",
      "[Epoch 4 | Step 3500] loss=7.2132\n",
      "Epoch 4 completed. Avg loss: 7.2116\n",
      "\n",
      "=== Epoch 5/5 ===\n",
      "[Epoch 5 | Step 1] loss=6.5972\n",
      "[Epoch 5 | Step 500] loss=6.9078\n",
      "[Epoch 5 | Step 1000] loss=6.9009\n",
      "[Epoch 5 | Step 1500] loss=6.8848\n",
      "[Epoch 5 | Step 2000] loss=6.8697\n",
      "[Epoch 5 | Step 2500] loss=6.8654\n",
      "[Epoch 5 | Step 3000] loss=6.8556\n",
      "[Epoch 5 | Step 3500] loss=6.8473\n",
      "Epoch 5 completed. Avg loss: 6.8470\n"
     ]
    }
   ],
   "source": [
    "#no negative sampling yet\n",
    "EPOCHS = 5\n",
    "LOG_EVERY = 500  # print every N steps\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{EPOCHS} ===\")\n",
    "    total_loss = 0.0\n",
    "    num_steps = 0\n",
    "\n",
    "    for step, (centers_tensor, contexts_tensor) in enumerate(dataloader, start=1):\n",
    "        centers_tensor  = centers_tensor.to(device).long()\n",
    "        contexts_tensor = contexts_tensor.to(device).long()\n",
    "\n",
    "        logits = model(centers_tensor)             \n",
    "        loss   = criterion(logits, contexts_tensor)  \n",
    "\n",
    "        optimiser.zero_grad() \n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "       \n",
    "        num_steps += 1\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if step % LOG_EVERY == 0 or step == 1:\n",
    "            avg_so_far = total_loss / num_steps\n",
    "            print(f\"[Epoch {epoch} | Step {step}] loss={avg_so_far:.4f}\")\n",
    "\n",
    "    epoch_avg = total_loss / max(1, num_steps)\n",
    "    print(f\"Epoch {epoch} completed. Avg loss: {epoch_avg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723e6c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('before', 0.5946093797683716), ('ignore', 0.5721971392631531), ('back', 0.5666858553886414), ('thread', 0.5606597065925598), ('through', 0.5575634241104126)]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "NEIGHBOUR_TOP_K = 5\n",
    "@torch.no_grad()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def nearest_neighbors(\n",
    "    model: SkipGram,\n",
    "    word_to_idx: Dict[str, int],\n",
    "    idx_to_word: Dict[int, str],\n",
    "    device: torch.device,\n",
    "    query_word: str,\n",
    "    top_k: int = NEIGHBOUR_TOP_K,\n",
    "):\n",
    "    model.eval()\n",
    "    if query_word not in word_to_idx:\n",
    "        print(f\"'{query_word}' not in vocab\")\n",
    "        return []\n",
    "\n",
    "    '''\n",
    "    we use input embeddings matrix because in the training, we take the center word get a score for all the context words\n",
    "    and so in the end, we have a trained input embeddings matrix that is used to find the nearest neighbors\n",
    "    '''\n",
    "    embed = model.input_embeddings.weight \n",
    "    q_idx = word_to_idx[query_word]\n",
    "    q_vec = embed[q_idx].unsqueeze(0)     \n",
    "\n",
    "    sims = F.cosine_similarity(q_vec, embed, dim=1)  #compare how similar  our query word is to all the other rows\n",
    "    sims[q_idx] = float(\"-inf\")  # don’t return self\n",
    "    top_vals, top_inds = torch.topk(sims, k=min(top_k, embed.size(0) - 1)) #get top k values and indices\n",
    "\n",
    "\n",
    "    return [(idx_to_word[i.item()], top_vals[j].item()) for j, i in enumerate(top_inds)]\n",
    "\n",
    "\n",
    "example='day'\n",
    "print(nearest_neighbors(model, word_to_idx, idx_to_word, device, example, top_k=NEIGHBOUR_TOP_K))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
