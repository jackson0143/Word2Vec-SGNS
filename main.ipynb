{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a70d4439",
   "metadata": {},
   "source": [
    "## Word2Vec from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748ffc8d",
   "metadata": {},
   "source": [
    "We will be building a Word2Vec method\n",
    "- What it is? It helps create the vector representation of a given word, called word embeddings.\n",
    "- Why is it useful? The vectors we create will aim to capture semantic meanings and their relationships with different words, so the famous example 'king' and 'queen' will end up in a similar vector space.\n",
    "\n",
    "There's two ways to do it:\n",
    "1. Continuous Bag of Words (CBOW) which tries to predict a word in the sentence, given its surrounding neighbour words.\n",
    "eg: the quick brown _____ jumps over the lazy dog. We can try to use our surrounding words of 'brown' and 'jumps' to try to predict the missing word 'fox'.\n",
    "\n",
    "2. Skip Gram is the reverse, instead it will predict the neighbours of a given word. \n",
    "eg: the quick ______ fox ______ over the lazy dog.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd73f1c",
   "metadata": {},
   "source": [
    "## Step 1: setup the dataset\n",
    "\n",
    "I'll go with the reddit text corpus today from Convokit. Its supposed stats are:\n",
    "\n",
    "- Number of Utterances: 297132\n",
    "- Number of Speakers: 119889\n",
    "- Number of Conversations: 8286\n",
    "\n",
    "We'll import the dataset then see what we're working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8b8a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import Corpus, download\n",
    "corpus = Corpus(filename=download(\"reddit-corpus-small\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "641c9a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Talk about your day. Anything goes, but subreddit rules still apply. Please be polite to each other! \\n', 'I went to visit a few days ago and Ioved it. I can’t find any negatives other than how small the place is. I’m also just a visitor so the perspective is entirely different from someone who lives there. ', 'One time, my family and I had just returned from Japan and we needed a big cab to load up all our baggage. So this prime MPV turned up and he refused to take us because we live in Tampines. On top  that, he was extremely rude. Plus, he started arguing with the neighbouring taxi Drivers and he airport Marshalls promptly told him to leave, which he did.    \\n   \\nLuckily another MPV taxi turned up, and the driver his round was SUPER friendly.', 'Talk about your day. Anything goes, but subreddit rules still apply. Please be polite to each other! \\n']\n"
     ]
    }
   ],
   "source": [
    "chosen_utts = []\n",
    "for i, utt in enumerate(corpus.iter_utterances()):\n",
    "    if i >= 4:\n",
    "        break\n",
    "    chosen_utts.append(utt.text)\n",
    "print(chosen_utts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b0fbfc",
   "metadata": {},
   "source": [
    "Okay we have chosen one line, lets proceed with working with this line to get a feel of how this works. Lets tokenise it to ensure we remove any ambiguities in variations of words that might trip up the model. eg: lowercase the words so 'Talk' and 'talk' aren't differentiated during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3de8a2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['talk', 'about', 'your', 'day', 'anything', 'goes', 'but', 'subreddit', 'rules', 'still', 'apply', 'please', 'be', 'polite', 'to', 'each', 'other'], ['i', 'went', 'to', 'visit', 'a', 'few', 'days', 'ago', 'and', 'ioved', 'it', 'i', 'can', 't', 'find', 'any', 'negatives', 'other', 'than', 'how', 'small', 'the', 'place', 'is', 'i', 'm', 'also', 'just', 'a', 'visitor', 'so', 'the', 'perspective', 'is', 'entirely', 'different', 'from', 'someone', 'who', 'lives', 'there'], ['one', 'time', 'my', 'family', 'and', 'i', 'had', 'just', 'returned', 'from', 'japan', 'and', 'we', 'needed', 'a', 'big', 'cab', 'to', 'load', 'up', 'all', 'our', 'baggage', 'so', 'this', 'prime', 'mpv', 'turned', 'up', 'and', 'he', 'refused', 'to', 'take', 'us', 'because', 'we', 'live', 'in', 'tampines', 'on', 'top', 'that', 'he', 'was', 'extremely', 'rude', 'plus', 'he', 'started', 'arguing', 'with', 'the', 'neighbouring', 'taxi', 'drivers', 'and', 'he', 'airport', 'marshalls', 'promptly', 'told', 'him', 'to', 'leave', 'which', 'he', 'did', 'luckily', 'another', 'mpv', 'taxi', 'turned', 'up', 'and', 'the', 'driver', 'his', 'round', 'was', 'super', 'friendly'], ['talk', 'about', 'your', 'day', 'anything', 'goes', 'but', 'subreddit', 'rules', 'still', 'apply', 'please', 'be', 'polite', 'to', 'each', 'other']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def tokenize(text: str):\n",
    "    return re.findall(r\"[a-z0-9]+'[a-z0-9]+|[a-z0-9]+\", text.lower(), flags=re.I)\n",
    "chosen_utts_cleaned = [tokenize(utt) for utt in chosen_utts]\n",
    "print(chosen_utts_cleaned)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43f32ca",
   "metadata": {},
   "source": [
    "Now with each utterance cleaned and broken down into simple tokens, we can map it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "330b4de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "talk\n",
      "[[97, 41, 77, 57, 96, 56, 9, 86, 17, 67, 24, 75, 31, 104, 100, 42, 29], [78, 79, 100, 71, 68, 38, 2, 5, 33, 36, 50, 78, 88, 4, 43, 47, 16, 29, 40, 46, 21, 69, 102, 55, 78, 80, 22, 0, 68, 3, 76, 69, 13, 55, 84, 103, 20, 52, 92, 99, 25], [58, 89, 48, 98, 33, 78, 1, 0, 59, 20, 34, 33, 61, 11, 68, 106, 32, 100, 28, 12, 87, 74, 30, 76, 6, 35, 83, 95, 12, 33, 62, 39, 100, 18, 53, 60, 61, 49, 45, 91, 7, 90, 10, 62, 64, 63, 85, 73, 62, 93, 72, 65, 69, 70, 105, 15, 33, 62, 44, 54, 19, 101, 26, 100, 82, 94, 62, 14, 37, 51, 83, 105, 95, 12, 33, 69, 27, 81, 23, 64, 66, 8], [97, 41, 77, 57, 96, 56, 9, 86, 17, 67, 24, 75, 31, 104, 100, 42, 29]]\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(chosen_utts_cleaned):\n",
    "    # Flatten tokenized utterances into a single list of words\n",
    "    words = [token for utterance in chosen_utts_cleaned for token in utterance]\n",
    "\n",
    "    vocab = set(words)\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "    return words, word_to_idx, idx_to_word\n",
    "\n",
    "\n",
    "# Convert tokens to integer ids using the built mapping\n",
    "def tokens_to_ids(chosen_utts_cleaned, word_to_idx):\n",
    "    return [[word_to_idx[w] for w in utt if w in word_to_idx] for utt in chosen_utts_cleaned]\n",
    "\n",
    "words, word_to_idx, idx_to_word = build_vocab(chosen_utts_cleaned)\n",
    "print(word_to_idx['days'])\n",
    "print(idx_to_word[97])\n",
    "chosen_utts_ids = tokens_to_ids(chosen_utts_cleaned, word_to_idx)\n",
    "print(chosen_utts_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aa0120",
   "metadata": {},
   "source": [
    "# Step 2: Define the Skip-gram pairs. \n",
    "\n",
    "Lets say for example we had a window size of 2:\n",
    "\n",
    "tokens = [\"quick\", \"brown\", \"fox\", \"jumps\", \"over\"]\n",
    "                0        1      2       3        4\n",
    "\n",
    "For this iteration, we choose 'fox' as the center word.\n",
    "window size tells us how far left and right we can look for the context words, so we can make 2 steps up until index 0 or index 4.\n",
    "\n",
    "Then all the words within index 0 and index 4 will be considered our **context** words (not including center word).\n",
    "Then we just make pairs for all of them, with the pairs being **(center, context)**\n",
    "\n",
    "Eg:(fox, quick), (fox, brown), (fox, jumps), (fox, over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cef84b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is what the pairs look like for the first utterance: [('talk', 'about'), ('talk', 'your'), ('about', 'talk'), ('about', 'your'), ('about', 'day'), ('your', 'talk'), ('your', 'about'), ('your', 'day'), ('your', 'anything'), ('day', 'about'), ('day', 'your'), ('day', 'anything'), ('day', 'goes'), ('anything', 'your'), ('anything', 'day'), ('anything', 'goes'), ('anything', 'but'), ('goes', 'day'), ('goes', 'anything'), ('goes', 'but'), ('goes', 'subreddit'), ('but', 'anything'), ('but', 'goes'), ('but', 'subreddit'), ('but', 'rules'), ('subreddit', 'goes'), ('subreddit', 'but'), ('subreddit', 'rules'), ('subreddit', 'still'), ('rules', 'but'), ('rules', 'subreddit'), ('rules', 'still'), ('rules', 'apply'), ('still', 'subreddit'), ('still', 'rules'), ('still', 'apply'), ('still', 'please'), ('apply', 'rules'), ('apply', 'still'), ('apply', 'please'), ('apply', 'be'), ('please', 'still'), ('please', 'apply'), ('please', 'be'), ('please', 'polite'), ('be', 'apply'), ('be', 'please'), ('be', 'polite'), ('be', 'to'), ('polite', 'please'), ('polite', 'be'), ('polite', 'to'), ('polite', 'each'), ('to', 'be'), ('to', 'polite'), ('to', 'each'), ('to', 'other'), ('each', 'polite'), ('each', 'to'), ('each', 'other'), ('other', 'to'), ('other', 'each')]\n",
      "this is what it looks like for all utterances and in ID form: [(97, 41), (97, 77), (41, 97), (41, 77), (41, 57), (77, 97), (77, 41), (77, 57), (77, 96), (57, 41), (57, 77), (57, 96), (57, 56), (96, 77), (96, 57), (96, 56), (96, 9), (56, 57), (56, 96), (56, 9), (56, 86), (9, 96), (9, 56), (9, 86), (9, 17), (86, 56), (86, 9), (86, 17), (86, 67), (17, 9), (17, 86), (17, 67), (17, 24), (67, 86), (67, 17), (67, 24), (67, 75), (24, 17), (24, 67), (24, 75), (24, 31), (75, 67), (75, 24), (75, 31), (75, 104), (31, 24), (31, 75), (31, 104), (31, 100), (104, 75), (104, 31), (104, 100), (104, 42), (100, 31), (100, 104), (100, 42), (100, 29), (42, 104), (42, 100), (42, 29), (29, 100), (29, 42), (78, 79), (78, 100), (79, 78), (79, 100), (79, 71), (100, 78), (100, 79), (100, 71), (100, 68), (71, 79), (71, 100), (71, 68), (71, 38), (68, 100), (68, 71), (68, 38), (68, 2), (38, 71), (38, 68), (38, 2), (38, 5), (2, 68), (2, 38), (2, 5), (2, 33), (5, 38), (5, 2), (5, 33), (5, 36), (33, 2), (33, 5), (33, 36), (33, 50), (36, 5), (36, 33), (36, 50), (36, 78), (50, 33), (50, 36), (50, 78), (50, 88), (78, 36), (78, 50), (78, 88), (78, 4), (88, 50), (88, 78), (88, 4), (88, 43), (4, 78), (4, 88), (4, 43), (4, 47), (43, 88), (43, 4), (43, 47), (43, 16), (47, 4), (47, 43), (47, 16), (47, 29), (16, 43), (16, 47), (16, 29), (16, 40), (29, 47), (29, 16), (29, 40), (29, 46), (40, 16), (40, 29), (40, 46), (40, 21), (46, 29), (46, 40), (46, 21), (46, 69), (21, 40), (21, 46), (21, 69), (21, 102), (69, 46), (69, 21), (69, 102), (69, 55), (102, 21), (102, 69), (102, 55), (102, 78), (55, 69), (55, 102), (55, 78), (55, 80), (78, 102), (78, 55), (78, 80), (78, 22), (80, 55), (80, 78), (80, 22), (80, 0), (22, 78), (22, 80), (22, 0), (22, 68), (0, 80), (0, 22), (0, 68), (0, 3), (68, 22), (68, 0), (68, 3), (68, 76), (3, 0), (3, 68), (3, 76), (3, 69), (76, 68), (76, 3), (76, 69), (76, 13), (69, 3), (69, 76), (69, 13), (69, 55), (13, 76), (13, 69), (13, 55), (13, 84), (55, 69), (55, 13), (55, 84), (55, 103), (84, 13), (84, 55), (84, 103), (84, 20), (103, 55), (103, 84), (103, 20), (103, 52), (20, 84), (20, 103), (20, 52), (20, 92), (52, 103), (52, 20), (52, 92), (52, 99), (92, 20), (92, 52), (92, 99), (92, 25), (99, 52), (99, 92), (99, 25), (25, 92), (25, 99), (58, 89), (58, 48), (89, 58), (89, 48), (89, 98), (48, 58), (48, 89), (48, 98), (48, 33), (98, 89), (98, 48), (98, 33), (98, 78), (33, 48), (33, 98), (33, 78), (33, 1), (78, 98), (78, 33), (78, 1), (78, 0), (1, 33), (1, 78), (1, 0), (1, 59), (0, 78), (0, 1), (0, 59), (0, 20), (59, 1), (59, 0), (59, 20), (59, 34), (20, 0), (20, 59), (20, 34), (20, 33), (34, 59), (34, 20), (34, 33), (34, 61), (33, 20), (33, 34), (33, 61), (33, 11), (61, 34), (61, 33), (61, 11), (61, 68), (11, 33), (11, 61), (11, 68), (11, 106), (68, 61), (68, 11), (68, 106), (68, 32), (106, 11), (106, 68), (106, 32), (106, 100), (32, 68), (32, 106), (32, 100), (32, 28), (100, 106), (100, 32), (100, 28), (100, 12), (28, 32), (28, 100), (28, 12), (28, 87), (12, 100), (12, 28), (12, 87), (12, 74), (87, 28), (87, 12), (87, 74), (87, 30), (74, 12), (74, 87), (74, 30), (74, 76), (30, 87), (30, 74), (30, 76), (30, 6), (76, 74), (76, 30), (76, 6), (76, 35), (6, 30), (6, 76), (6, 35), (6, 83), (35, 76), (35, 6), (35, 83), (35, 95), (83, 6), (83, 35), (83, 95), (83, 12), (95, 35), (95, 83), (95, 12), (95, 33), (12, 83), (12, 95), (12, 33), (12, 62), (33, 95), (33, 12), (33, 62), (33, 39), (62, 12), (62, 33), (62, 39), (62, 100), (39, 33), (39, 62), (39, 100), (39, 18), (100, 62), (100, 39), (100, 18), (100, 53), (18, 39), (18, 100), (18, 53), (18, 60), (53, 100), (53, 18), (53, 60), (53, 61), (60, 18), (60, 53), (60, 61), (60, 49), (61, 53), (61, 60), (61, 49), (61, 45), (49, 60), (49, 61), (49, 45), (49, 91), (45, 61), (45, 49), (45, 91), (45, 7), (91, 49), (91, 45), (91, 7), (91, 90), (7, 45), (7, 91), (7, 90), (7, 10), (90, 91), (90, 7), (90, 10), (90, 62), (10, 7), (10, 90), (10, 62), (10, 64), (62, 90), (62, 10), (62, 64), (62, 63), (64, 10), (64, 62), (64, 63), (64, 85), (63, 62), (63, 64), (63, 85), (63, 73), (85, 64), (85, 63), (85, 73), (85, 62), (73, 63), (73, 85), (73, 62), (73, 93), (62, 85), (62, 73), (62, 93), (62, 72), (93, 73), (93, 62), (93, 72), (93, 65), (72, 62), (72, 93), (72, 65), (72, 69), (65, 93), (65, 72), (65, 69), (65, 70), (69, 72), (69, 65), (69, 70), (69, 105), (70, 65), (70, 69), (70, 105), (70, 15), (105, 69), (105, 70), (105, 15), (105, 33), (15, 70), (15, 105), (15, 33), (15, 62), (33, 105), (33, 15), (33, 62), (33, 44), (62, 15), (62, 33), (62, 44), (62, 54), (44, 33), (44, 62), (44, 54), (44, 19), (54, 62), (54, 44), (54, 19), (54, 101), (19, 44), (19, 54), (19, 101), (19, 26), (101, 54), (101, 19), (101, 26), (101, 100), (26, 19), (26, 101), (26, 100), (26, 82), (100, 101), (100, 26), (100, 82), (100, 94), (82, 26), (82, 100), (82, 94), (82, 62), (94, 100), (94, 82), (94, 62), (94, 14), (62, 82), (62, 94), (62, 14), (62, 37), (14, 94), (14, 62), (14, 37), (14, 51), (37, 62), (37, 14), (37, 51), (37, 83), (51, 14), (51, 37), (51, 83), (51, 105), (83, 37), (83, 51), (83, 105), (83, 95), (105, 51), (105, 83), (105, 95), (105, 12), (95, 83), (95, 105), (95, 12), (95, 33), (12, 105), (12, 95), (12, 33), (12, 69), (33, 95), (33, 12), (33, 69), (33, 27), (69, 12), (69, 33), (69, 27), (69, 81), (27, 33), (27, 69), (27, 81), (27, 23), (81, 69), (81, 27), (81, 23), (81, 64), (23, 27), (23, 81), (23, 64), (23, 66), (64, 81), (64, 23), (64, 66), (64, 8), (66, 23), (66, 64), (66, 8), (8, 64), (8, 66), (97, 41), (97, 77), (41, 97), (41, 77), (41, 57), (77, 97), (77, 41), (77, 57), (77, 96), (57, 41), (57, 77), (57, 96), (57, 56), (96, 77), (96, 57), (96, 56), (96, 9), (56, 57), (56, 96), (56, 9), (56, 86), (9, 96), (9, 56), (9, 86), (9, 17), (86, 56), (86, 9), (86, 17), (86, 67), (17, 9), (17, 86), (17, 67), (17, 24), (67, 86), (67, 17), (67, 24), (67, 75), (24, 17), (24, 67), (24, 75), (24, 31), (75, 67), (75, 24), (75, 31), (75, 104), (31, 24), (31, 75), (31, 104), (31, 100), (104, 75), (104, 31), (104, 100), (104, 42), (100, 31), (100, 104), (100, 42), (100, 29), (42, 104), (42, 100), (42, 29), (29, 100), (29, 42)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "def make_skip_gram_pairs(tokens, window, dynamic_window: bool = False):\n",
    "    pairs = []\n",
    "    for i, center in enumerate(tokens):\n",
    "        #random weight if dynamic_window is true\n",
    "        w = random.randint(1, window) if dynamic_window else window\n",
    "\n",
    "        #make all combinations of (center, context) pairs\n",
    "        left_pointer = max(0, i- w)\n",
    "        right_pointer = min(len(tokens)-1, i+w)\n",
    "        for j in range(left_pointer, right_pointer+1):\n",
    "            if j == i:\n",
    "                continue\n",
    "            pairs.append((center, tokens[j]))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "#we can make pairs for all utterances, and return as a single list flattened since we're using a dataset class\n",
    "def make_pairs_for_all_utterances(chosen_utts_ids, window, dynamic_window: bool = False):\n",
    "    all_pairs = []\n",
    "    for utt in chosen_utts_ids:\n",
    "        all_pairs.extend(make_skip_gram_pairs(utt, window, dynamic_window))\n",
    "    return all_pairs\n",
    "\n",
    "\n",
    "window = 2\n",
    "first_cleaned_utterance =  chosen_utts_cleaned[0]\n",
    "print(f\"this is what the pairs look like for the first utterance: {make_skip_gram_pairs(first_cleaned_utterance, window)}\")\n",
    "print(f\"this is what it looks like for all utterances and in ID form: {make_pairs_for_all_utterances(chosen_utts_ids, window)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e685504",
   "metadata": {},
   "source": [
    "We laod the data into a dataset class from PyTorch, so we can run as batches later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e960e178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "training_data = make_pairs_for_all_utterances(chosen_utts_ids, window)\n",
    "dataset = Word2VecDataset(training_data)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8372458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
